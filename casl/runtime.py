# casl/runtime.py
"""
CASL Runtime — helpers available to generated addons at runtime.

This module can be imported by generated code for more complex operations
that are awkward to inline.
"""

from __future__ import annotations

from typing import Any, Callable, Dict, List, Optional, Set, Tuple


class CASLEnvironment:
    """
    Runtime environment for a CASL addon execution.

    Holds analysis caches, suppression state, and shared data
    between checkers within the same addon.
    """

    def __init__(self):
        self.analyses: Dict[str, Any] = {}
        self.suppressions: Set[str] = set()
        self.file_suppressions: Dict[str, str] = {}
        self.globals: Dict[str, Any] = {}
        self._analysis_cache: Dict[Tuple[str, int], Any] = {}

    def get_analysis(self, name: str, cfg: Any) -> Any:
        """
        Get or lazily compute a dataflow analysis for the given configuration.

        Supported analysis names:
          - "reaching_defs"
          - "live_vars"
          - "intervals"
          - "null_ptrs"
          - "taint"
          - "signs"
          - "constants"
          - "pointers"
        """
        cache_key = (name, id(cfg))
        if cache_key in self._analysis_cache:
            return self._analysis_cache[cache_key]

        # Lazy import to avoid circular deps
        try:
            from cppcheckdata_shims.dataflow_analyses import (
                ReachingDefinitions, LiveVariables, IntervalAnalysis,
                NullPointerAnalysis, TaintAnalysis, SignAnalysis,
                ConstantPropagation, PointerAnalysis,
            )

            analysis_map = {
                "reaching_defs": ReachingDefinitions,
                "live_vars": LiveVariables,
                "intervals": IntervalAnalysis,
                "null_ptrs": NullPointerAnalysis,
                "taint": TaintAnalysis,
                "signs": SignAnalysis,
                "constants": ConstantPropagation,
                "pointers": PointerAnalysis,
            }

            cls = analysis_map.get(name)
            if cls is None:
                raise ValueError(f"Unknown analysis: {name}")

            analysis = cls(cfg)
            analysis.run()
            self._analysis_cache[cache_key] = analysis
            return analysis

        except ImportError as exc:
            raise RuntimeError(
                f"cppcheckdata_shims is required for analysis '{name}': {exc}"
            ) from exc

    def suppress(self, error_id: str, file_glob: Optional[str] = None):
        if file_glob:
            self.file_suppressions[error_id] = file_glob
        else:
            self.suppressions.add(error_id)

    def is_suppressed(self, error_id: str, file: str = "") -> bool:
        if error_id in self.suppressions:
            return True
        from fnmatch import fnmatch
        for eid, pattern in self.file_suppressions.items():
            if eid == error_id and fnmatch(file, pattern):
                return True
        return False


class PatternMatcher:
    """
    Runtime pattern matching engine for PQL-style patterns.

    This provides more sophisticated matching than the inline code
    generated by the compiler — used for complex patterns with
    backtracking and capture groups.
    """

    def __init__(self):
        self.captures: Dict[str, Any] = {}

    def match_token(self, tok: Any, **constraints) -> bool:
        """Match a single token against constraints."""
        for key, val in constraints.items():
            actual = getattr(tok, key, None)
            if callable(val):
                if not val(actual):
                    return False
            elif actual != val:
                return False
        return True

    def match_sequence(self, start_tok: Any, patterns: List[dict]) -> Optional[List[Any]]:
        """
        Match a sequence of token patterns starting from start_tok.
        Returns list of matched tokens, or None if no match.
        """
        matched = []
        tok = start_tok
        for pattern in patterns:
            if tok is None:
                return None
            if not self.match_token(tok, **pattern):
                return None
            matched.append(tok)
            tok = getattr(tok, 'next', None)
        return matched

    def find_all(self, cfg: Any, pattern_fn: Callable) -> List[Any]:
        """Find all tokens in cfg matching the given pattern function."""
        results = []
        for tok in getattr(cfg, 'tokenlist', []):
            if pattern_fn(tok):
                results.append(tok)
        return results

    def find_calls_to(self, cfg: Any, func_name: str) -> List[Any]:
        """Find all call sites to the named function."""
        results = []
        for tok in getattr(cfg, 'tokenlist', []):
            if getattr(tok, 'str', None) == '(' and getattr(tok, 'previous', None):
                prev = tok.previous
                if getattr(prev, 'str', None) == func_name:
                    results.append(tok)
        return results

    def find_assignments_to(self, cfg: Any, var_name: str) -> List[Any]:
        """Find all assignments to the named variable."""
        results = []
        for tok in getattr(cfg, 'tokenlist', []):
            if getattr(tok, 'isAssignmentOp', False):
                op1 = getattr(tok, 'astOperand1', None)
                if op1 and getattr(op1, 'str', None) == var_name:
                    results.append(tok)
        return results


class TraceAnalyzer:
    """
    PQL-inspired trace analyzer for interprocedural pattern matching.

    Analyzes sequences of operations (allocations, uses, frees, etc.)
    to detect resource lifecycle violations.
    """

    def __init__(self, cfg: Any):
        self.cfg = cfg
        self._traces: Dict[int, List[Tuple[str, Any]]] = {}  # varId → [(action, token)]

    def build_traces(self, actions: Dict[str, Callable]):
        """
        Build traces for all variables.

        :param actions: Maps action names to predicates on tokens.
                        E.g., {"alloc": is_alloc, "free": is_free, "use": is_use}
        """
        for tok in getattr(self.cfg, 'tokenlist', []):
            var_id = getattr(tok, 'varId', None)
            if var_id is None:
                continue
            for action_name, predicate in actions.items():
                if predicate(tok):
                    if var_id not in self._traces:
                        self._traces[var_id] = []
                    self._traces[var_id].append((action_name, tok))

    def find_violations(self, pattern: List[str]) -> List[Tuple[int, List[Any]]]:
        """
        Find variables whose trace contains the given action sequence.

        E.g., find_violations(["free", "use"]) finds use-after-free.
        Returns list of (varId, [matching_tokens]).
        """
        results = []
        for var_id, trace in self._traces.items():
            actions = [t[0] for t in trace]
            tokens = [t[1] for t in trace]
            # Simple subsequence match
            match_tokens = self._subsequence_match(actions, tokens, pattern)
            if match_tokens:
                results.append((var_id, match_tokens))
        return results

    def _subsequence_match(
        self, actions: List[str], tokens: List[Any], pattern: List[str]
    ) -> Optional[List[Any]]:
        """Find first subsequence of actions matching pattern."""
        matched = []
        pi = 0
        for i, action in enumerate(actions):
            if pi < len(pattern) and action == pattern[pi]:
                matched.append(tokens[i])
                pi += 1
                if pi == len(pattern):
                    return matched
        return None if pi < len(pattern) else matched
